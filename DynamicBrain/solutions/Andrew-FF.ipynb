{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec #Useful tool to arrange multiple plots in one figure (https://matplotlib.org/stable/api/_as_gen/matplotlib.gridspec.GridSpec.html)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "data_dirname = 'visual-behavior-neuropixels'\n",
    "use_static = False\n",
    "if 'Darwin' in platstring or 'macOS' in platstring:\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2021/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on AWS\n",
    "    data_root = \"/data/\"\n",
    "    data_dirname = 'visual-behavior-neuropixels-data'\n",
    "    use_static = True\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/home/andrew/Documents/tmp/5-1-24-change-detection\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "\n",
    "# this path determines where downloaded data will be stored\n",
    "manifest_path = os.path.join(data_root, \"manifest.json\")\n",
    "cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n",
    "\n",
    "print(cache.get_all_session_types())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from allensdk.brain_observatory.behavior.behavior_project_cache.\\\n",
    "#     behavior_neuropixels_project_cache \\\n",
    "#     import VisualBehaviorNeuropixelsProjectCache\n",
    "\n",
    "# from allensdk.brain_observatory.behavior.\n",
    "\n",
    "# # this path should point to the location of the dataset on your platform\n",
    "# cache_dir = os.path.join(data_root, data_dirname)\n",
    "# cache = VisualCodingNeuropixelsProjectCache.from_s3_cache(cache_dir=cache_dir)\n",
    "\n",
    "# # cache = VisualBehaviorNeuropixelsProjectCache.from_local_cache(\n",
    "# #             cache_dir=cache_dir, use_static_cache=use_static)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = cache.get_session_table()\n",
    "brain_observatory_type_sessions = sessions[sessions[\"session_type\"] == \"brain_observatory_1.1\"]\n",
    "brain_observatory_type_sessions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvatory_type_sessions = sessions[sessions[\"session_type\"] == \"brain_observatory_1.1\"]\n",
    "brain_observatory_type_sessions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvatory_type_sessions = sessions[sessions[\"session_type\"] == \"brain_observatory_1.1\"]\n",
    "brain_observatory_type_sessions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = 791319847\n",
    "session = cache.get_session_data(session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.width', None)\n",
    "print(session.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.structurewise_unit_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_stimulus_table('flashes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentations = session.get_stimulus_table(\"flashes\")\n",
    "units = session.units[session.units[\"ecephys_structure_acronym\"] == 'VISp']\n",
    "\n",
    "time_step = 0.01\n",
    "time_bins = np.arange(-0.1, 0.5 + time_step, time_step)\n",
    "\n",
    "histograms = session.presentationwise_spike_counts(\n",
    "    stimulus_presentation_ids=presentations.index.values,  \n",
    "    bin_edges=time_bins,\n",
    "    unit_ids=units.index.values\n",
    ")\n",
    "\n",
    "histograms.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_histograms = histograms.mean(dim=\"stimulus_presentation_id\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.pcolormesh(\n",
    "    mean_histograms[\"time_relative_to_stimulus_onset\"], \n",
    "    np.arange(mean_histograms[\"unit_id\"].size),\n",
    "    mean_histograms.T, \n",
    "    vmin=0,\n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"unit\", fontsize=24)\n",
    "ax.set_xlabel(\"time relative to stimulus onset (s)\", fontsize=24)\n",
    "ax.set_title(\"peristimulus time histograms for VISp units on flash presentations\", fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_presentations = session.get_stimulus_table(\"natural_scenes\")\n",
    "visp_units = session.units[session.units[\"ecephys_structure_acronym\"] == \"VISp\"]\n",
    "\n",
    "spikes = session.presentationwise_spike_times(\n",
    "    stimulus_presentation_ids=scene_presentations.index.values,\n",
    "    unit_ids=visp_units.index.values[:]\n",
    ")\n",
    "\n",
    "spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes[\"count\"] = np.zeros(spikes.shape[0])\n",
    "spikes = spikes.groupby([\"stimulus_presentation_id\", \"unit_id\"]).count()\n",
    "\n",
    "design = pd.pivot_table(\n",
    "    spikes, \n",
    "    values=\"count\", \n",
    "    index=\"stimulus_presentation_id\", \n",
    "    columns=\"unit_id\", \n",
    "    fill_value=0.0,\n",
    "    aggfunc=np.sum\n",
    ")\n",
    "\n",
    "design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = scene_presentations.loc[design.index.values, \"frame\"]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.get_stimulus_table(\"flashes\").head(3)\n",
    "session.units.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_units = session.units[session.units['ecephys_structure_acronym'].isin(regions)]\n",
    "# visual_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regions = ['VISp', 'VISl', 'VISrl', 'VISal', 'VISpm', 'VISam']  # Initial focus on VIS areas\n",
    "start_time = 1280\n",
    "end_time = 1400\n",
    "# start_time = 1288\n",
    "# end_time = 1292\n",
    "\n",
    "visual_units = session.units[session.units['ecephys_structure_acronym'].isin(regions)]\n",
    "\n",
    "for unit_id, times in session.spike_times.items():\n",
    "    if unit_id in visual_units.index:\n",
    "        condition = (session.spike_times[unit_id] > start_time) & (session.spike_times[unit_id] < end_time)\n",
    "        print(session.spike_times[unit_id][condition])\n",
    "\n",
    "# session.spike_times\n",
    "\n",
    "# for key in session.spike_times.keys():\n",
    "#     print(session.spike_times[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detailed_spike_data(session, start_time, end_time, regions):\n",
    "    \"\"\"\n",
    "    Extract detailed spike timing data for specific regions within a defined time window.\n",
    "\n",
    "    Parameters:\n",
    "        session (EcephysSession): The session object containing the unit and spike data.\n",
    "        start_time (float): Start time of the window.\n",
    "        end_time (float): End time of the window.\n",
    "        regions (list): List of regions to include in the analysis.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Detailed spike timing data for each unit, including the unit's region.\n",
    "    \"\"\"\n",
    "    # Filter units based on specified regions\n",
    "    visual_units = session.units[session.units['ecephys_structure_acronym'].isin(regions)]\n",
    "\n",
    "    # Prepare a DataFrame to store results\n",
    "    spikes_list = []\n",
    "    for unit_id, times in session.spike_times.items():\n",
    "        if unit_id in visual_units.index:\n",
    "            # Filter spike times within the time window\n",
    "            relevant_spikes = times[(times >= start_time) & (times <= end_time)]\n",
    "            for spike_time in relevant_spikes:\n",
    "                spikes_list.append({\n",
    "                    'unit_id': unit_id,\n",
    "                    'spike_time': spike_time,\n",
    "                    'region': visual_units.loc[unit_id, 'ecephys_structure_acronym']\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(spikes_list)\n",
    "\n",
    "# Example usage\n",
    "start_time = 1288\n",
    "end_time = 1292\n",
    "detailed_spike_data = extract_detailed_spike_data(session, start_time, end_time, regions)\n",
    "detailed_spike_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_detailed_spike_data(session, start_time, end_time, regions):\n",
    "    \"\"\"\n",
    "    Extract detailed spike timing data for specific regions within a defined time window.\n",
    "\n",
    "    Parameters:\n",
    "        session (EcephysSession): The session object containing the unit and spike data.\n",
    "        start_time (float): Start time of the window.\n",
    "        end_time (float): End time of the window.\n",
    "        regions (list): List of regions to include in the analysis.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Detailed spike timing data for each unit, including the unit's region.\n",
    "    \"\"\"\n",
    "    # Filter units based on specified regions\n",
    "    visual_units = session.units[session.units['ecephys_structure_acronym'].isin(regions)]\n",
    "\n",
    "    # Prepare a DataFrame to store results\n",
    "    spikes_list = []\n",
    "    for unit_id, times in session.spike_times.items():\n",
    "        if unit_id in visual_units.index:\n",
    "            # Filter spike times within the time window\n",
    "            relevant_spikes = times[(times >= start_time) & (times <= end_time)]\n",
    "            for spike_time in relevant_spikes:\n",
    "                spikes_list.append({\n",
    "                    'unit_id': unit_id,\n",
    "                    'spike_time': spike_time,\n",
    "                    'region': visual_units.loc[unit_id, 'ecephys_structure_acronym']\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(spikes_list)\n",
    "\n",
    "\n",
    "def plot_firing_rates_over_time(spike_data, bin_size, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Plot firing rates over time for each region in the spike data.\n",
    "\n",
    "    Parameters:\n",
    "        spike_data (DataFrame): DataFrame containing 'spike_time' and 'region' columns.\n",
    "        bin_size (float): Size of each time bin in seconds.\n",
    "        start_time (float): Start time of the window for plotting.\n",
    "        end_time (float): End time of the window for plotting.\n",
    "    \"\"\"\n",
    "    # Creating the bins\n",
    "    bins = np.arange(start_time, end_time, bin_size)\n",
    "    regions = spike_data['region'].unique()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Binning and plotting for each region\n",
    "    for region in regions:\n",
    "        region_spikes = spike_data[spike_data['region'] == region]['spike_time']\n",
    "        spike_counts, _ = np.histogram(region_spikes, bins=bins)\n",
    "        firing_rates = spike_counts / bin_size  # Convert counts to rates\n",
    "        \n",
    "        # Plotting\n",
    "        plt.plot(bins[:-1] + bin_size/2, firing_rates, label=f'Region: {region}')\n",
    "    \n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Firing Rate (spikes/s)')\n",
    "    plt.title('Firing Rates Over Time by Region')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# start_time = 1288\n",
    "# end_time = 1292\n",
    "start_time = 1291\n",
    "end_time = 1293\n",
    "detailed_spike_data = extract_detailed_spike_data(session, start_time, end_time, regions)\n",
    "\n",
    "# Assuming you've defined bin_size, start_time, and end_time\n",
    "bin_size = 0.05  # Example bin size of 100ms\n",
    "plot_firing_rates_over_time(detailed_spike_data, bin_size, start_time, end_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.units.columns\n",
    "count_structure_acronyms = session.units['structure_acronym'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.stimulus_presentations.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def get_time_window(session, buffer=1):\n",
    "    \"\"\"\n",
    "    Get the time window from slightly before the first flash to slightly after the third flash.\n",
    "    \n",
    "    Parameters:\n",
    "        session (EcephysSession): The session object.\n",
    "        buffer (float): Buffer time in seconds to extend before the first and after the last flash.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: start_time, end_time\n",
    "    \"\"\"\n",
    "    num_flashes = 50\n",
    "    flashes = session.get_stimulus_table(\"flashes\").head(num_flashes)\n",
    "    print(\"number of flashes!!!-------------\")\n",
    "    print(len(flashes))\n",
    "    num_flashes = min(num_flashes, len(flashes))\n",
    "    start_time = flashes.iloc[0]['start_time'] - buffer\n",
    "    end_time = flashes.iloc[num_flashes-1]['stop_time'] + buffer\n",
    "\n",
    "    flash_start_times = flashes['start_time'].values\n",
    "    flash_end_times = flashes['stop_time'].values\n",
    "    print(flash_start_times)\n",
    "\n",
    "    return start_time, end_time, flash_start_times, flash_end_times\n",
    "\n",
    "def extract_and_bin_spikes(session, start_time, end_time, bin_size, regions, flash_start_times, flash_end_times):\n",
    "    \"\"\"\n",
    "    Extract spikes for each region within a defined time window and bin them.\n",
    "    \n",
    "    Parameters:\n",
    "        session (EcephysSession): The session object.\n",
    "        start_time (float): Start time of the window.\n",
    "        end_time (float): End time of the window.\n",
    "        bin_size (float): Bin size in seconds.\n",
    "        regions (list): List of regions to include in the analysis.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing binned firing rates for each region.\n",
    "    \"\"\"\n",
    "    binned_flash_starts, _ = np.histogram(flash_start_times, bins=np.arange(start_time, end_time, bin_size))\n",
    "    binned_flash_ends, _ = np.histogram(flash_end_times, bins=np.arange(start_time, end_time, bin_size))\n",
    "\n",
    "    region_data = {region: [] for region in regions}\n",
    "\n",
    "    # print(\"spike times\")\n",
    "    # print(session.spike_times)\n",
    "    # print(\"units\")\n",
    "    # print(session.units.columns)\n",
    "    for unit_id, spikes in session.spike_times.items():\n",
    "        region = session.units.loc[unit_id, 'ecephys_structure_acronym']\n",
    "        if region in regions:\n",
    "            binned_spikes, times = np.histogram(spikes, bins=np.arange(start_time, end_time, bin_size))\n",
    "            # region_data[region].append(pd.Series(binned_spikes / bin_size, name=unit_id, index=times[:-1]))\n",
    "            region_data[region].append(pd.Series(binned_spikes / bin_size, name=unit_id))\n",
    "\n",
    "    # Average across units in each region or provide zeros where no data exists\n",
    "    for region in regions:\n",
    "        if region_data[region]:\n",
    "            region_data[region] = pd.concat(region_data[region], axis=1).mean(axis=1)\n",
    "        else:\n",
    "            # Initialize the series with zeros for each time binS\n",
    "            print(\"------------------------alert missing region data: \", region)\n",
    "            # print(region)\n",
    "            region_data[region] = pd.Series(np.zeros(len(np.arange(start_time, end_time, bin_size)[:-1])))\n",
    "\n",
    "    return region_data, binned_flash_starts, binned_flash_ends\n",
    "\n",
    "\n",
    "# def plot_firing_rates(region_data, bin_size, average_flash_starts, average_flash_ends):\n",
    "#     \"\"\"\n",
    "#     Plot the firing rates over time for each region, ensuring data alignment.\n",
    "\n",
    "#     Parameters:\n",
    "#         region_data (dict): Dictionary containing firing rates binned over time for each region.\n",
    "#         bin_size (float): Bin size used in seconds.\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "\n",
    "#     for region, data in region_data.items():\n",
    "#         if not data.empty:\n",
    "#             # Generate time bins based on the data index\n",
    "#             time_bins = np.linspace(data.index[0], data.index[-1], num=len(data))\n",
    "            \n",
    "#             # Plotting each region's data\n",
    "#             plt.plot(time_bins, data.values, label=f'Region: {region}')\n",
    "#         else:\n",
    "#             # Handle cases where there is no data for a region\n",
    "#             print(f\"---------------No data available for region {region}\")\n",
    "\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Firing Rate (spikes/s)')\n",
    "#     plt.title('Average Firing Rate Over Time by Region Across Selected Sessions')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "# Main analysis for two selected sessions\n",
    "sessions = cache.get_session_table()\n",
    "brain_observatory_type_sessions = sessions[sessions[\"session_type\"] == \"brain_observatory_1.1\"]\n",
    "# selected_sessions = brain_observatory_type_sessions.head(600)  # Select only the first two sessions\n",
    "selected_sessions = sessions.head(600)  # Select only the first two sessions\n",
    "print(len(selected_sessions))\n",
    "\n",
    "regions = [\"LGd\", \"LGv\", \"LP\", \"VISp\", \"VISl\", \"VISal\", \"VISrl\", \"VISpm\", \"VISam\"]\n",
    "bin_size = 0.005  # Example bin size of 100ms\n",
    "\n",
    "# Collect data from all selected sessions\n",
    "per_unit_counts = {region: [] for region in regions}\n",
    "all_region_data = {region: [] for region in regions}\n",
    "blank_regions = {region: 0 for region in regions}\n",
    "all_binned_flash_starts = []\n",
    "all_binned_flash_ends = []\n",
    "count = -1\n",
    "for session_id in selected_sessions.index:\n",
    "    count += 1\n",
    "\n",
    "    # # skip known bad sessions that don't have all regions\n",
    "    # if count in [0, 3, 4, 5, 6, 7]:\n",
    "    #     continue\n",
    "\n",
    "\n",
    "\n",
    "    session = cache.get_session_data(session_id)\n",
    "\n",
    "    count_structure_acronyms = session.units['structure_acronym'].value_counts()\n",
    "    for region in regions:\n",
    "        per_unit_counts[region].append(count_structure_acronyms.get(region, 0))\n",
    "        # print(f\"Region: {region}, Count: {count_structure_acronyms.get(region, 0)}\")\n",
    "\n",
    "    start_time, end_time, flash_start_times, flash_end_times = get_time_window(session)\n",
    "    print(\"start_time, end_time\")\n",
    "    print(start_time, end_time)\n",
    "    region_data, binned_flash_starts, binned_flash_ends = extract_and_bin_spikes(session, start_time, end_time, bin_size, regions, flash_start_times, flash_end_times)\n",
    "\n",
    "    if not all(len(df) == 20066 for df in region_data.values()):\n",
    "        print(\"BIG PROBLEM BINS DON't MATCH: \", len(region_data[\"VISp\"]))\n",
    "        break\n",
    "\n",
    "    for region in regions:\n",
    "        if not region_data[region].any():\n",
    "            print(f\"{region} bumping empty count for region\", region_data[region])\n",
    "            blank_regions[region] += 1\n",
    "\n",
    "    # should_continue = False\n",
    "    # for region in regions:\n",
    "    #     if not region_data[region].any():\n",
    "    #         print(f\"{region} region_data[region].any(): \", region_data[region])\n",
    "    #         print(\"------skipping where no data: \", region)\n",
    "    #         should_continue = True\n",
    "    #         break\n",
    "    # if should_continue:\n",
    "    #     continue\n",
    "\n",
    "    for region in regions:\n",
    "        all_region_data[region].append(region_data[region])\n",
    "\n",
    "    all_binned_flash_starts.append(binned_flash_starts)\n",
    "    all_binned_flash_ends.append(binned_flash_ends)\n",
    "\n",
    "    # # TODO: delete this  \n",
    "    # if count == 2:\n",
    "    #     break\n",
    "\n",
    "# print(all_region_data[\"VISp\"])\n",
    "\n",
    "# print(f\"region_data: { region_data }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions[\"session_type\"].value_counts()\n",
    "connectivity_sessions = sessions[sessions[\"session_type\"] == \"functional_connectivity\"]\n",
    "connectivity_sessions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Assuming all_region_data is correctly populated\n",
    "# average_region_data = {region: pd.concat(all_region_data[region], axis=1).mean(axis=1) if all_region_data[region] else pd.Series() for region in regions}\n",
    "# plot_firing_rates(average_region_data, bin_size)\n",
    "\n",
    "# Assuming all_region_data is correctly populated\n",
    "# print(all_region_data[\"VISp\"])\n",
    "\n",
    "def plot_firing_rates(region_data, bin_size, average_flash_starts, average_flash_ends, flash_start_offset):\n",
    "    \"\"\"\n",
    "    Plot the firing rates over time for each region, ensuring data alignment,\n",
    "    and add vertical lines for average flash starts based on a binary indicator.\n",
    "\n",
    "    Parameters:\n",
    "        region_data (dict): Dictionary containing firing rates binned over time for each region.\n",
    "        bin_size (float): Bin size used in seconds.\n",
    "        flash_indicator (array): Array of binary values indicating flash starts.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for region, data in region_data.items():\n",
    "        if not data.empty:\n",
    "            # Generate time bins based on the data index\n",
    "            time_bins = np.linspace(data.index[0], data.index[-1], num=len(data))\n",
    "            \n",
    "            # Plotting each region's data\n",
    "            plt.plot(time_bins, data.values, label=f'Region: {region}')\n",
    "        else:\n",
    "            # Handle cases where there is no data for a region\n",
    "            print(f\"No data available for region {region}\")\n",
    "\n",
    "    # Calculate the actual times of flash starts from the indicator\n",
    "    # flash_starts = np.where(average_flash_starts == 1)[0] * bin_size\n",
    "\n",
    "    # Plot vertical lines for average flash starts\n",
    "    for bin_num, flash_start in enumerate(average_flash_starts):\n",
    "        if flash_start == 1:\n",
    "            plt.axvline(x=flash_start_offset, color='r', linestyle='--', label='Flash Start' if 'Flash Start' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "        \n",
    "        flash_start_offset += 1\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Firing Rate (spikes/s)')\n",
    "    plt.title('Average Firing Rate Over Time by Region Across Selected Sessions')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "truncate_pre = 0\n",
    "truncate = 10000000000\n",
    "\n",
    "all_binned_flash_starts_cp = []\n",
    "for i in range(len(all_binned_flash_starts)):\n",
    "    all_binned_flash_starts_cp.append(all_binned_flash_starts[i][truncate_pre:truncate])\n",
    "\n",
    "average_flash_starts = np.mean(all_binned_flash_starts_cp, axis=0)\n",
    "average_flash_ends = np.mean(all_binned_flash_ends, axis=0)\n",
    "# print(average_flash_starts)\n",
    "\n",
    "print(len(all_region_data[\"VISp\"]))\n",
    "print(len(all_region_data[\"VISp\"][0]))\n",
    "print()\n",
    "flash_indices = np.where(average_flash_starts == 1)\n",
    "print(flash_indices)\n",
    "print(len(flash_indices))\n",
    "print(len(flash_indices[0]))\n",
    "num_flashes = len(flash_indices)\n",
    "all_region_data_cp = {}\n",
    "for region in regions:\n",
    "    all_region_data_cp[region] = []\n",
    "    for series in all_region_data[region]:\n",
    "        all_region_data_cp[region].append(series[truncate_pre:truncate])\n",
    "\n",
    "average_region_data_cp = {region: pd.concat(all_region_data_cp[region], axis=1).mean(axis=1) for region in regions}\n",
    "# print(average_region_data[\"VISam\"].head(100))\n",
    "plot_firing_rates(average_region_data_cp, bin_size, average_flash_starts, average_flash_ends, flash_start_offset=truncate_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def plot_firing_rates(region_data, bin_size, average_flash_starts, average_flash_ends, flash_offset, std_err_data):\n",
    "    \"\"\"\n",
    "    Plot the firing rates over time for each region, ensuring data alignment,\n",
    "    and add vertical lines for average flash starts and ends based on a binary indicator.\n",
    "\n",
    "    Parameters:\n",
    "        region_data (dict): Dictionary containing firing rates binned over time for each region.\n",
    "        bin_size (float): Bin size used in seconds.\n",
    "        average_flash_starts (array): Array of binary values indicating flash starts.\n",
    "        average_flash_ends (array): Array of binary values indicating flash ends.\n",
    "        flash_offset (float): Offset to align the flash indicators correctly with the time bins.\n",
    "    \"\"\"\n",
    "    print(average_flash_starts - average_flash_ends)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Define a blue colormap that scales from dark to light\n",
    "    colors = LinearSegmentedColormap.from_list(\"blue_grad\", [\"lightblue\", \"darkblue\"], N=9)\n",
    "    color_map = {region: colors(i) for i, region in enumerate(regions)}\n",
    "\n",
    "    # Define the subset range you are interested in for debugging\n",
    "    # start_bin = 230  # Change this to your start bin\n",
    "    # end_bin = 280    # Change this to your end bin\n",
    "\n",
    "    start_bin = 257 * 1\n",
    "    end_bin = 280 * 1\n",
    "\n",
    "    # start_bin = 0\n",
    "    # end_bin = 1000\n",
    "\n",
    "    regions_for_fn = [\"VISp\", \"VISl\", \"VISal\", \"VISrl\", \"VISpm\", \"VISam\"]\n",
    "\n",
    "    for region_0 in regions_for_fn:\n",
    "\n",
    "        for i, (region, data) in enumerate(region_data.items()):\n",
    "            if region not in [region_0]:\n",
    "            # if region not in [\"VISal\"]:\n",
    "                continue\n",
    "\n",
    "            if not data.empty:\n",
    "                # Generate time bins based on the data index\n",
    "                time_bins = np.linspace(data.index[0], data.index[-1], num=len(data))\n",
    "                \n",
    "                # Slice time bins and data for the range you are interested in\n",
    "                sliced_time_bins = time_bins[start_bin:end_bin]\n",
    "                sliced_data = data.values[start_bin:end_bin]\n",
    "\n",
    "                # Get the standard error for the sliced data\n",
    "                std_err = std_err_data[region].values[start_bin:end_bin]\n",
    "\n",
    "                # TODO: assign variable that is std_err to `std_err`\n",
    "\n",
    "                # Plotting each region's data with hierarchical color\n",
    "                plt.plot(sliced_time_bins, sliced_data, label=f'Region: {region}', color=color_map[region])\n",
    "                plt.fill_between(sliced_time_bins, sliced_data - std_err, sliced_data + std_err, alpha=0.3, color=color_map[region])\n",
    "            else:\n",
    "                # Handle cases where there is no data for a region\n",
    "                print(f\"No data available for region {region}\")\n",
    "\n",
    "        # Plot vertical lines for average flash starts and ends within the subset\n",
    "        for bin_num, (flash_start, flash_end) in enumerate(zip(average_flash_starts[start_bin:end_bin], average_flash_ends[start_bin:end_bin])):\n",
    "            if flash_start == 1:\n",
    "                plt.axvline(x=flash_offset + bin_num + start_bin, color='r', linestyle='--', label='Flash Start' if 'Flash Start' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "            if flash_end == 1:\n",
    "                plt.axvline(x=flash_offset + bin_num + start_bin, color='b', linestyle='--', label='Flash End' if 'Flash End' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "        plt.xlabel('Time (bins)')\n",
    "        plt.ylabel('Firing Rate (spikes/s)')\n",
    "        plt.title('Average Firing Rate Over Time by Region Across Selected Sessions')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    for i, (region, data) in enumerate(region_data.items()):\n",
    "        if region not in regions_for_fn:\n",
    "            continue\n",
    "\n",
    "        if not data.empty:\n",
    "            # Generate time bins based on the data index\n",
    "            time_bins = np.linspace(data.index[0], data.index[-1], num=len(data))\n",
    "            \n",
    "            # Slice time bins and data for the range you are interested in\n",
    "            sliced_time_bins = time_bins[start_bin:end_bin]\n",
    "            sliced_data = data.values[start_bin:end_bin]\n",
    "\n",
    "            # Get the standard error for the sliced data\n",
    "            std_err = std_err_data[region].values[start_bin:end_bin]\n",
    "\n",
    "            # TODO: assign variable that is std_err to `std_err`\n",
    "\n",
    "            # Plotting each region's data with hierarchical color\n",
    "            plt.plot(sliced_time_bins, sliced_data, label=f'Region: {region}', color=color_map[region])\n",
    "            plt.fill_between(sliced_time_bins, sliced_data - std_err, sliced_data + std_err, alpha=0.3, color=color_map[region])\n",
    "        else:\n",
    "            # Handle cases where there is no data for a region\n",
    "            print(f\"No data available for region {region}\")\n",
    "\n",
    "    # Plot vertical lines for average flash starts and ends within the subset\n",
    "    for bin_num, (flash_start, flash_end) in enumerate(zip(average_flash_starts[start_bin:end_bin], average_flash_ends[start_bin:end_bin])):\n",
    "        if flash_start == 1:\n",
    "            plt.axvline(x=flash_offset + bin_num + start_bin, color='r', linestyle='--', label='Flash Start' if 'Flash Start' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "        if flash_end == 1:\n",
    "            plt.axvline(x=flash_offset + bin_num + start_bin, color='b', linestyle='--', label='Flash End' if 'Flash End' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "    plt.xlabel('Time (bins)')\n",
    "    plt.ylabel('Firing Rate (spikes/s)')\n",
    "    plt.title('Average Firing Rate Over Time by Region Across Selected Sessions')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "regions = [\"LGd\", \"LGv\", \"LP\", \"VISp\", \"VISl\", \"VISal\", \"VISrl\", \"VISpm\", \"VISam\"]\n",
    "\n",
    "# for region in regions:\n",
    "# # blank_regions[region]) / count_structure_acronyms[region]\n",
    "#     if region not in blank_regions:\n",
    "#         blank_regions[region] = 0\n",
    "#     if region not in count_structure_acronyms:\n",
    "#         print(\"WARN: region not in count_structure_acronyms: \", region)\n",
    "#         count_structure_acronyms[region] = 0\n",
    "\n",
    "# determine from flash starts how many timesteps before first flash\n",
    "num_time_steps_before_first_flash = np.where(average_flash_starts == 1)[0][0]\n",
    "# print(num_time_steps_before_first_flash)\n",
    "\n",
    "# determine from flash starts how many timesteps after last flash\n",
    "num_time_steps_after_last_flash = len(average_flash_starts) - np.where(average_flash_starts == 1)[0][-1]\n",
    "# print(num_time_steps_after_last_flash)\n",
    "\n",
    "\n",
    "truncate_pre = 0\n",
    "truncate = 100000\n",
    "\n",
    "all_binned_flash_starts_cp = []\n",
    "for i in range(len(all_binned_flash_starts)):\n",
    "    all_binned_flash_starts_cp.append(all_binned_flash_starts[i][truncate_pre:truncate])\n",
    "\n",
    "average_flash_starts = np.mean(all_binned_flash_starts_cp, axis=0)\n",
    "average_flash_ends = np.mean(all_binned_flash_ends, axis=0)\n",
    "# print(average_flash_starts)\n",
    "\n",
    "# print(len(all_region_data[\"VISp\"]))\n",
    "# print(len(all_region_data[\"VISp\"][0]))\n",
    "# print()\n",
    "all_region_data_cp = {}\n",
    "for region in regions:\n",
    "    all_region_data_cp[region] = []\n",
    "    for series in all_region_data[region]:\n",
    "        all_region_data_cp[region].append(series[truncate_pre:truncate])\n",
    "\n",
    "# average_region_data_cp = {region: pd.concat(all_region_data_cp[region], axis=1).mean(axis=1) for region in regions}\n",
    "\n",
    "    # iterate through all region data cp\n",
    "        # for each region data cp[region][session][index] divide by number of units\n",
    "\n",
    "# normalize based on number of units\n",
    "normalized_region_data_cp = {}\n",
    "std_err_data = {}\n",
    "for region in all_region_data_cp:\n",
    "    region_data_normalized_by_nuits = []\n",
    "    for i, session_data in enumerate(all_region_data_cp[region]):\n",
    "        build = []\n",
    "        for measurement in session_data:\n",
    "            if per_unit_counts[region][i] == 0:\n",
    "                assert measurement == 0\n",
    "                build.append(0)\n",
    "            else:\n",
    "                build.append(measurement/per_unit_counts[region][i])\n",
    "        region_data_normalized_by_nuits.append(pd.Series(build))\n",
    "    normalized_region_data_cp[region] = region_data_normalized_by_nuits\n",
    "\n",
    "    # TODO: perform this calculation without zeros in the dataset\n",
    "    # Calculate the standard error after normalization and before averaging\n",
    "    concatenated_normalized_data = pd.concat(normalized_region_data_cp[region], axis=1)\n",
    "    non_zero_columns = concatenated_normalized_data.loc[:, (concatenated_normalized_data != 0).any(axis=0)]\n",
    "    # print(concatenated_normalized_data.shape)\n",
    "    # print(non_zero_columns.shape)\n",
    "\n",
    "    # std_err_data[region] = non_zero_columns\n",
    "\n",
    "    # std_err_data[region] = non_zero_columns.std(axis=1) / np.sqrt(non_zero_columns.shape[1])\n",
    "    std_err_data[region] = non_zero_columns\n",
    "\n",
    "    # \n",
    "    # print(\"-----------\")\n",
    "    # print(std_err_data[region].shape)\n",
    "    # # print(std_err_data[region][0])\n",
    "    # # print(std_err_data[region][1])\n",
    "    # print(concatenated_normalized_data.shape[1])\n",
    "    # print(concatenated_normalized_data.std(axis=1).shape)\n",
    "    # print(\"++++++++\")\n",
    "    # print(normalized_region_data_cp[region][0].shape)\n",
    "    # print(std_err_data[region][0])\n",
    "    # print(normalized_region_data_cp[region][0].shape)\n",
    "\n",
    "\n",
    "# account for blank regions\n",
    "average_region_data_cp = {}\n",
    "for region in normalized_region_data_cp:\n",
    "    num_sessions = len(normalized_region_data_cp[region])\n",
    "    # average_region_data_cp[region] = pd.concat(normalized_region_data_cp[region], axis=1).sum(axis=1)\n",
    "    average_region_data_cp[region] = pd.concat(normalized_region_data_cp[region], axis=1)\n",
    "    non_zero_columns_blank_calcs = average_region_data_cp[region].loc[:, (average_region_data_cp[region] != 0).any(axis=0)]\n",
    "    average_region_data_cp[region] = non_zero_columns_blank_calcs.sum(axis=1) / non_zero_columns_blank_calcs.shape[1]\n",
    "    # print(f\"num sessions / blank regions: {num_sessions} / {blank_regions[region]}\")\n",
    "    # for i in range(0, len(average_region_data_cp[region])):\n",
    "    #     average_region_data_cp[region][i] = average_region_data_cp[region][i] / (num_sessions - blank_regions[region])\n",
    "\n",
    "flash_indices = np.where(average_flash_starts == 1)[0]\n",
    "num_flashes = len(flash_indices)\n",
    "print(\"num flashes: \", num_flashes)\n",
    "chunk_size = num_time_steps_before_first_flash + num_time_steps_after_last_flash\n",
    "\n",
    "print(\"Number of time steps before first flash:\", num_time_steps_before_first_flash)\n",
    "print(\"Number of time steps after last flash:\", num_time_steps_after_last_flash)\n",
    "print(\"Chunk size:\", chunk_size)\n",
    "\n",
    "print(\"Length of average_region_data_cp['VISam']:\", len(average_region_data_cp[\"VISam\"]))\n",
    "\n",
    "\n",
    "\n",
    "overlaid_region_data = {}\n",
    "for region in regions:\n",
    "    overlaid_chunks = []\n",
    "    # IMPORTANT: how flashes to ignore\n",
    "    for i in range(3, num_flashes):\n",
    "        start_index = flash_indices[i] - num_time_steps_before_first_flash\n",
    "        end_index = start_index + chunk_size\n",
    "        chunk = average_region_data_cp[region].iloc[start_index:end_index].reset_index(drop=True)  # Reset index here\n",
    "        overlaid_chunks.append(chunk)\n",
    "\n",
    "    overlaid_chunks_concat = pd.concat(overlaid_chunks, axis=1)\n",
    "    num_chunks = overlaid_chunks_concat.shape[1]\n",
    "    overlaid_chunks_sum = overlaid_chunks_concat.sum(axis=1)\n",
    "    for i in range(len(overlaid_chunks_sum)):\n",
    "        overlaid_chunks_sum[i] = overlaid_chunks_sum[i] / (num_chunks)\n",
    "\n",
    "    overlaid_region_data[region] = overlaid_chunks_sum\n",
    "\n",
    "    # trim the last few elements off of std_err_data for region\n",
    "    std_err_data[region] = std_err_data[region].iloc[0:chunk_size * num_flashes]\n",
    "\n",
    "    # calculate std err by overlapping chunks\n",
    "    # print(std_err_data[region].shape)\n",
    "    # print(std_err_data[region].shape[0] % chunk_size)\n",
    "    # print(std_err_data[region].shape[1])\n",
    "    std_err_data[region] = std_err_data[region].values.reshape(-1, chunk_size, std_err_data[region].shape[1])\n",
    "    std_err_data[region] = std_err_data[region].transpose(2, 0, 1)\n",
    "    shape_for_std_err_calc = std_err_data[region].shape\n",
    "    std_err_data[region] = std_err_data[region].reshape(shape_for_std_err_calc[0] * shape_for_std_err_calc[1], shape_for_std_err_calc[2])\n",
    "    std_err_data[region] = pd.DataFrame(std_err_data[region])\n",
    "    print(\"---\")\n",
    "    print(std_err_data[region].shape)\n",
    "    std_err_data[region] = std_err_data[region].std(axis=0) / np.sqrt(std_err_data[region].shape[0])\n",
    "    print(std_err_data[region].shape)\n",
    "    print(\"---\")\n",
    "\n",
    "# print(\"VISpm: \", overlaid_region_data[\"VISpm\"])\n",
    "\n",
    "# TODO: This needs to move on top in beginning before we calculate std_err!!!!!!!!\n",
    "# # by region, average overlaid region data by the average over timesteps 0-190\n",
    "# region_averages = {}\n",
    "# end_average_timestep = 180\n",
    "# for region in regions:\n",
    "#     running_sum = 0\n",
    "#     for i in range(0, end_average_timestep):\n",
    "#         running_sum += overlaid_region_data[region][i]\n",
    "#     average = running_sum / end_average_timestep\n",
    "#     region_averages[region] = average\n",
    "\n",
    "# # subtract by the average over timesteps 0-190\n",
    "# for region in regions:\n",
    "#     for i in range(0, len(overlaid_region_data[region])):\n",
    "#         overlaid_region_data[region][i] = overlaid_region_data[region][i] - region_averages[region]\n",
    "\n",
    "\n",
    "average_flash_starts = average_flash_starts[0:chunk_size]\n",
    "average_flash_ends = average_flash_ends[0:chunk_size]\n",
    "plot_firing_rates(overlaid_region_data, bin_size, average_flash_starts, average_flash_ends, flash_offset=truncate_pre, std_err_data=std_err_data)\n",
    "\n",
    "def find_peak_amplitude(data, margin):\n",
    "    max_value = data.max()\n",
    "    peak_amplitude = max_value\n",
    "    for i in range(len(data) - 1, -1, -1):\n",
    "        if data.iloc[i] >= max_value - margin:\n",
    "            peak_amplitude = data.iloc[i]\n",
    "            break\n",
    "    return peak_amplitude\n",
    "\n",
    "def find_time_to_percent_peak(data, peak_value, percent, start_bin, end_bin):\n",
    "    target_value = peak_value * percent\n",
    "    peak_reached = False\n",
    "    for i in range(len(data)):\n",
    "        if not peak_reached and data.iloc[i] == peak_value:\n",
    "            peak_reached = True\n",
    "        if peak_reached and data.iloc[i] <= target_value:\n",
    "            return data.index[i]\n",
    "    return -1\n",
    "\n",
    "start_bin = 257\n",
    "end_bin = 280\n",
    "margin = 0.0125  # Adjust this value according to your needs\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "percentages = np.arange(0.50, 0.96, 0.05)\n",
    "bar_width = 0.15\n",
    "opacity = 0.8\n",
    "\n",
    "regions = [\"VISp\", \"VISl\", \"VISal\", \"VISrl\", \"VISpm\", \"VISam\"]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(regions)))\n",
    "\n",
    "for i, region in enumerate(regions):\n",
    "    data = overlaid_region_data[region]\n",
    "    sliced_data = data[start_bin:end_bin]\n",
    "    peak_amplitude = find_peak_amplitude(sliced_data, margin)\n",
    "    time_to_percent_peak = []\n",
    "\n",
    "    for percent in percentages:\n",
    "        time = find_time_to_percent_peak(sliced_data, peak_amplitude, percent, start_bin, end_bin)\n",
    "        if time != -1:\n",
    "            time_to_percent_peak.append((time - start_bin) * bin_size)\n",
    "        else:\n",
    "            time_to_percent_peak.append(np.nan)\n",
    "\n",
    "    index = np.arange(len(percentages))\n",
    "    plt.bar(index + i * bar_width, time_to_percent_peak, bar_width, alpha=opacity, color=colors[i], label=f'Region: {region}')\n",
    "\n",
    "plt.xlabel('Percentage of Peak Amplitude (%)')\n",
    "plt.ylabel('Time After Stimulus (s)')\n",
    "plt.title('Time to Reach Percentage of Peak Amplitude by Region')\n",
    "plt.xticks(index + bar_width * (len(regions) - 1) / 2, (percentages * 100).astype(int))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def plot_firing_rates(region_data, bin_size, average_flash_starts, average_flash_ends, flash_offset, std_err_data):\n",
    "    \"\"\"\n",
    "    Plot the firing rates over time for each region, ensuring data alignment,\n",
    "    and add vertical lines for average flash starts and ends based on a binary indicator.\n",
    "\n",
    "    Parameters:\n",
    "        region_data (dict): Dictionary containing firing rates binned over time for each region.\n",
    "        bin_size (float): Bin size used in seconds.\n",
    "        average_flash_starts (array): Array of binary values indicating flash starts.\n",
    "        average_flash_ends (array): Array of binary values indicating flash ends.\n",
    "        flash_offset (float): Offset to align the flash indicators correctly with the time bins.\n",
    "    \"\"\"\n",
    "    print(average_flash_starts - average_flash_ends)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Define a blue colormap that scales from dark to light\n",
    "    colors = LinearSegmentedColormap.from_list(\"blue_grad\", [\"lightblue\", \"darkblue\"], N=9)\n",
    "    color_map = {region: colors(i) for i, region in enumerate(regions)}\n",
    "\n",
    "    # Define the subset range you are interested in for debugging\n",
    "    # start_bin = 230  # Change this to your start bin\n",
    "    # end_bin = 280    # Change this to your end bin\n",
    "\n",
    "    start_bin = 257 * 1\n",
    "    end_bin = 280 * 1\n",
    "\n",
    "    # start_bin = 0\n",
    "    # end_bin = 1000\n",
    "\n",
    "    regions_for_fn = [\"VISp\", \"VISl\", \"VISal\", \"VISrl\", \"VISpm\", \"VISam\"]\n",
    "\n",
    "    for region_0 in regions_for_fn:\n",
    "\n",
    "        for i, (region, data) in enumerate(region_data.items()):\n",
    "            if region not in [region_0]:\n",
    "            # if region not in [\"VISal\"]:\n",
    "                continue\n",
    "\n",
    "            if not data.empty:\n",
    "                # Generate time bins based on the data index\n",
    "                time_bins = np.linspace(data.index[0], data.index[-1], num=len(data))\n",
    "                \n",
    "                # Slice time bins and data for the range you are interested in\n",
    "                sliced_time_bins = time_bins[start_bin:end_bin]\n",
    "                sliced_data = data.values[start_bin:end_bin]\n",
    "\n",
    "                # Get the standard error for the sliced data\n",
    "                std_err = std_err_data[region].values[start_bin:end_bin]\n",
    "\n",
    "                # TODO: assign variable that is std_err to `std_err`\n",
    "\n",
    "                # Plotting each region's data with hierarchical color\n",
    "                plt.plot(sliced_time_bins, sliced_data, label=f'Region: {region}', color=color_map[region])\n",
    "                plt.fill_between(sliced_time_bins, sliced_data - std_err, sliced_data + std_err, alpha=0.3, color=color_map[region])\n",
    "            else:\n",
    "                # Handle cases where there is no data for a region\n",
    "                print(f\"No data available for region {region}\")\n",
    "\n",
    "        # Plot vertical lines for average flash starts and ends within the subset\n",
    "        for bin_num, (flash_start, flash_end) in enumerate(zip(average_flash_starts[start_bin:end_bin], average_flash_ends[start_bin:end_bin])):\n",
    "            if flash_start == 1:\n",
    "                plt.axvline(x=flash_offset + bin_num + start_bin, color='r', linestyle='--', label='Flash Start' if 'Flash Start' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "            if flash_end == 1:\n",
    "                plt.axvline(x=flash_offset + bin_num + start_bin, color='b', linestyle='--', label='Flash End' if 'Flash End' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "        plt.xlabel('Time (bins)')\n",
    "        plt.ylabel('Firing Rate (spikes/s)')\n",
    "        plt.title('Average Firing Rate Over Time by Region Across Selected Sessions')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    for i, (region, data) in enumerate(region_data.items()):\n",
    "        if region not in regions_for_fn:\n",
    "            continue\n",
    "\n",
    "        if not data.empty:\n",
    "            # Generate time bins based on the data index\n",
    "            time_bins = np.linspace(data.index[0], data.index[-1], num=len(data))\n",
    "            \n",
    "            # Slice time bins and data for the range you are interested in\n",
    "            sliced_time_bins = time_bins[start_bin:end_bin]\n",
    "            sliced_data = data.values[start_bin:end_bin]\n",
    "\n",
    "            # Get the standard error for the sliced data\n",
    "            std_err = std_err_data[region].values[start_bin:end_bin]\n",
    "\n",
    "            # TODO: assign variable that is std_err to `std_err`\n",
    "\n",
    "            # Plotting each region's data with hierarchical color\n",
    "            plt.plot(sliced_time_bins, sliced_data, label=f'Region: {region}', color=color_map[region])\n",
    "            plt.fill_between(sliced_time_bins, sliced_data - std_err, sliced_data + std_err, alpha=0.3, color=color_map[region])\n",
    "        else:\n",
    "            # Handle cases where there is no data for a region\n",
    "            print(f\"No data available for region {region}\")\n",
    "\n",
    "    # Plot vertical lines for average flash starts and ends within the subset\n",
    "    for bin_num, (flash_start, flash_end) in enumerate(zip(average_flash_starts[start_bin:end_bin], average_flash_ends[start_bin:end_bin])):\n",
    "        if flash_start == 1:\n",
    "            plt.axvline(x=flash_offset + bin_num + start_bin, color='r', linestyle='--', label='Flash Start' if 'Flash Start' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "        if flash_end == 1:\n",
    "            plt.axvline(x=flash_offset + bin_num + start_bin, color='b', linestyle='--', label='Flash End' if 'Flash End' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "    plt.xlabel('Time (bins)')\n",
    "    plt.ylabel('Firing Rate (spikes/s)')\n",
    "    plt.title('Average Firing Rate Over Time by Region Across Selected Sessions')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "regions = [\"LGd\", \"LGv\", \"LP\", \"VISp\", \"VISl\", \"VISal\", \"VISrl\", \"VISpm\", \"VISam\"]\n",
    "\n",
    "# for region in regions:\n",
    "# # blank_regions[region]) / count_structure_acronyms[region]\n",
    "#     if region not in blank_regions:\n",
    "#         blank_regions[region] = 0\n",
    "#     if region not in count_structure_acronyms:\n",
    "#         print(\"WARN: region not in count_structure_acronyms: \", region)\n",
    "#         count_structure_acronyms[region] = 0\n",
    "\n",
    "# determine from flash starts how many timesteps before first flash\n",
    "num_time_steps_before_first_flash = np.where(average_flash_starts == 1)[0][0]\n",
    "# print(num_time_steps_before_first_flash)\n",
    "\n",
    "# determine from flash starts how many timesteps after last flash\n",
    "num_time_steps_after_last_flash = len(average_flash_starts) - np.where(average_flash_starts == 1)[0][-1]\n",
    "# print(num_time_steps_after_last_flash)\n",
    "\n",
    "\n",
    "truncate_pre = 0\n",
    "truncate = 100000\n",
    "\n",
    "all_binned_flash_starts_cp = []\n",
    "for i in range(len(all_binned_flash_starts)):\n",
    "    all_binned_flash_starts_cp.append(all_binned_flash_starts[i][truncate_pre:truncate])\n",
    "\n",
    "average_flash_starts = np.mean(all_binned_flash_starts_cp, axis=0)\n",
    "average_flash_ends = np.mean(all_binned_flash_ends, axis=0)\n",
    "# print(average_flash_starts)\n",
    "\n",
    "# print(len(all_region_data[\"VISp\"]))\n",
    "# print(len(all_region_data[\"VISp\"][0]))\n",
    "# print()\n",
    "all_region_data_cp = {}\n",
    "for region in regions:\n",
    "    all_region_data_cp[region] = []\n",
    "    for series in all_region_data[region]:\n",
    "        all_region_data_cp[region].append(series[truncate_pre:truncate])\n",
    "\n",
    "# average_region_data_cp = {region: pd.concat(all_region_data_cp[region], axis=1).mean(axis=1) for region in regions}\n",
    "\n",
    "    # iterate through all region data cp\n",
    "        # for each region data cp[region][session][index] divide by number of units\n",
    "\n",
    "# normalize based on number of units\n",
    "normalized_region_data_cp = {}\n",
    "std_err_data = {}\n",
    "for region in all_region_data_cp:\n",
    "    region_data_normalized_by_nuits = []\n",
    "    for i, session_data in enumerate(all_region_data_cp[region]):\n",
    "        build = []\n",
    "        for measurement in session_data:\n",
    "            if per_unit_counts[region][i] == 0:\n",
    "                assert measurement == 0\n",
    "                build.append(0)\n",
    "            else:\n",
    "                build.append(measurement/per_unit_counts[region][i])\n",
    "        region_data_normalized_by_nuits.append(pd.Series(build))\n",
    "    normalized_region_data_cp[region] = region_data_normalized_by_nuits\n",
    "\n",
    "    # TODO: perform this calculation without zeros in the dataset\n",
    "    # Calculate the standard error after normalization and before averaging\n",
    "    concatenated_normalized_data = pd.concat(normalized_region_data_cp[region], axis=1)\n",
    "    non_zero_columns = concatenated_normalized_data.loc[:, (concatenated_normalized_data != 0).any(axis=0)]\n",
    "    print(concatenated_normalized_data.shape)\n",
    "    print(non_zero_columns.shape)\n",
    "    std_err_data[region] = non_zero_columns.std(axis=1) / np.sqrt(non_zero_columns.shape[1])\n",
    "    print(\"-----------\")\n",
    "#     -----------\n",
    "# (12059,)\n",
    "# (12059, 58)\n",
    "# 58\n",
    "# (12059,)\n",
    "# ++++++++\n",
    "    print(std_err_data[region].shape)\n",
    "    # print(std_err_data[region][0])\n",
    "    # print(std_err_data[region][1])\n",
    "    print(concatenated_normalized_data.shape[1])\n",
    "    print(concatenated_normalized_data.std(axis=1).shape)\n",
    "    print(\"++++++++\")\n",
    "    print(normalized_region_data_cp[region][0].shape)\n",
    "    print(std_err_data[region][0])\n",
    "    print(normalized_region_data_cp[region][0].shape)\n",
    "\n",
    "\n",
    "# account for blank regions\n",
    "average_region_data_cp = {}\n",
    "for region in normalized_region_data_cp:\n",
    "    num_sessions = len(normalized_region_data_cp[region])\n",
    "    average_region_data_cp[region] = pd.concat(normalized_region_data_cp[region], axis=1).sum(axis=1)\n",
    "    print(f\"num sessions / blank regions: {num_sessions} / {blank_regions[region]}\")\n",
    "    for i in range(0, len(average_region_data_cp[region])):\n",
    "        average_region_data_cp[region][i] = average_region_data_cp[region][i] / (num_sessions - blank_regions[region])\n",
    "\n",
    "flash_indices = np.where(average_flash_starts == 1)[0]\n",
    "num_flashes = len(flash_indices)\n",
    "print(\"num flashes: \", num_flashes)\n",
    "chunk_size = num_time_steps_before_first_flash + num_time_steps_after_last_flash\n",
    "\n",
    "print(\"Number of time steps before first flash:\", num_time_steps_before_first_flash)\n",
    "print(\"Number of time steps after last flash:\", num_time_steps_after_last_flash)\n",
    "print(\"Chunk size:\", chunk_size)\n",
    "\n",
    "print(\"Length of average_region_data_cp['VISam']:\", len(average_region_data_cp[\"VISam\"]))\n",
    "\n",
    "\n",
    "\n",
    "overlaid_region_data = {}\n",
    "for region in regions:\n",
    "    overlaid_chunks = []\n",
    "    # IMPORTANT: how flashes to ignore\n",
    "    for i in range(3, num_flashes):\n",
    "        start_index = flash_indices[i] - num_time_steps_before_first_flash\n",
    "        end_index = start_index + chunk_size\n",
    "        chunk = average_region_data_cp[region].iloc[start_index:end_index].reset_index(drop=True)  # Reset index here\n",
    "        overlaid_chunks.append(chunk)\n",
    "\n",
    "    overlaid_chunks_concat = pd.concat(overlaid_chunks, axis=1)\n",
    "    num_chunks = overlaid_chunks_concat.shape[1]\n",
    "    overlaid_chunks_sum = overlaid_chunks_concat.sum(axis=1)\n",
    "    for i in range(len(overlaid_chunks_sum)):\n",
    "        overlaid_chunks_sum[i] = overlaid_chunks_sum[i] / (num_chunks)\n",
    "\n",
    "    overlaid_region_data[region] = overlaid_chunks_sum\n",
    "\n",
    "print(\"VISpm: \", overlaid_region_data[\"VISpm\"])\n",
    "\n",
    "# TODO: This needs to move on top in beginning before we calculate std_err!!!!!!!!\n",
    "# # by region, average overlaid region data by the average over timesteps 0-190\n",
    "# region_averages = {}\n",
    "# end_average_timestep = 180\n",
    "# for region in regions:\n",
    "#     running_sum = 0\n",
    "#     for i in range(0, end_average_timestep):\n",
    "#         running_sum += overlaid_region_data[region][i]\n",
    "#     average = running_sum / end_average_timestep\n",
    "#     region_averages[region] = average\n",
    "\n",
    "# # subtract by the average over timesteps 0-190\n",
    "# for region in regions:\n",
    "#     for i in range(0, len(overlaid_region_data[region])):\n",
    "#         overlaid_region_data[region][i] = overlaid_region_data[region][i] - region_averages[region]\n",
    "\n",
    "\n",
    "average_flash_starts = average_flash_starts[0:chunk_size]\n",
    "average_flash_ends = average_flash_ends[0:chunk_size]\n",
    "plot_firing_rates(overlaid_region_data, bin_size, average_flash_starts, average_flash_ends, flash_offset=truncate_pre, std_err_data=std_err_data)\n",
    "\n",
    "def find_peak_amplitude(data, margin):\n",
    "    max_value = data.max()\n",
    "    peak_amplitude = max_value\n",
    "    for i in range(len(data) - 1, -1, -1):\n",
    "        if data.iloc[i] >= max_value - margin:\n",
    "            peak_amplitude = data.iloc[i]\n",
    "            break\n",
    "    return peak_amplitude\n",
    "\n",
    "def find_time_to_percent_peak(data, peak_value, percent, start_bin, end_bin):\n",
    "    target_value = peak_value * percent\n",
    "    peak_reached = False\n",
    "    for i in range(len(data)):\n",
    "        if not peak_reached and data.iloc[i] == peak_value:\n",
    "            peak_reached = True\n",
    "        if peak_reached and data.iloc[i] <= target_value:\n",
    "            return data.index[i]\n",
    "    return -1\n",
    "\n",
    "start_bin = 257\n",
    "end_bin = 280\n",
    "margin = 0.0125  # Adjust this value according to your needs\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "percentages = np.arange(0.50, 0.96, 0.05)\n",
    "bar_width = 0.15\n",
    "opacity = 0.8\n",
    "\n",
    "regions = [\"VISp\", \"VISl\", \"VISal\", \"VISrl\", \"VISpm\", \"VISam\"]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(regions)))\n",
    "\n",
    "for i, region in enumerate(regions):\n",
    "    data = overlaid_region_data[region]\n",
    "    sliced_data = data[start_bin:end_bin]\n",
    "    peak_amplitude = find_peak_amplitude(sliced_data, margin)\n",
    "    time_to_percent_peak = []\n",
    "\n",
    "    for percent in percentages:\n",
    "        time = find_time_to_percent_peak(sliced_data, peak_amplitude, percent, start_bin, end_bin)\n",
    "        if time != -1:\n",
    "            time_to_percent_peak.append((time - start_bin) * bin_size)\n",
    "        else:\n",
    "            time_to_percent_peak.append(np.nan)\n",
    "\n",
    "    index = np.arange(len(percentages))\n",
    "    plt.bar(index + i * bar_width, time_to_percent_peak, bar_width, alpha=opacity, color=colors[i], label=f'Region: {region}')\n",
    "\n",
    "plt.xlabel('Percentage of Peak Amplitude (%)')\n",
    "plt.ylabel('Time After Stimulus (s)')\n",
    "plt.title('Time to Reach Percentage of Peak Amplitude by Region')\n",
    "plt.xticks(index + bar_width * (len(regions) - 1) / 2, (percentages * 100).astype(int))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `flashes_df` is your DataFrame from `session.get_stimulus_table(\"flashes\")`\n",
    "flashes_df = session.get_stimulus_table(\"flashes\")\n",
    "\n",
    "# Plotting the start and stop times of flashes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(flashes_df['start_time'][0:10], np.ones_like(flashes_df['start_time'][0:10]), 'go', label='Start Time')\n",
    "plt.plot(flashes_df['stop_time'][0:10], np.ones_like(flashes_df['stop_time'][0:10]), 'ro', label='Stop Time')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Start and Stop Times of Flash Stimuli')\n",
    "plt.yticks([])  # Hide y-axis labels as they are not informative here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.stimulus_presentations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.units.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_arr = design.values.astype(float)\n",
    "targets_arr = targets.values.astype(int)\n",
    "\n",
    "labels = np.unique(targets_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=5).split(design_arr):\n",
    "    \n",
    "    clf = svm.SVC(gamma=\"scale\", kernel=\"rbf\")\n",
    "    clf.fit(design_arr[train_indices], targets_arr[train_indices])\n",
    "    \n",
    "    test_targets = targets_arr[test_indices]\n",
    "    test_predictions = clf.predict(design_arr[test_indices])\n",
    "    \n",
    "    accuracy = 1 - (np.count_nonzero(test_predictions - test_targets) / test_predictions.size)\n",
    "    print(accuracy)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"chance: {1/labels.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_confusion = np.mean(confusions, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "img = ax.imshow(mean_confusion)\n",
    "fig.colorbar(img)\n",
    "\n",
    "ax.set_ylabel(\"actual\")\n",
    "ax.set_xlabel(\"predicted\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = labels[np.argmax(np.diag(mean_confusion))]\n",
    "worst = labels[np.argmin(np.diag(mean_confusion))]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "best_image = cache.get_natural_scene_template(best)\n",
    "ax[0].imshow(best_image, cmap=plt.cm.gray)\n",
    "ax[0].set_title(\"most decodable\", fontsize=24)\n",
    "\n",
    "worst_image = cache.get_natural_scene_template(worst)\n",
    "ax[1].imshow(worst_image, cmap=plt.cm.gray)\n",
    "ax[1].set_title(\"least decodable\", fontsize=24)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
